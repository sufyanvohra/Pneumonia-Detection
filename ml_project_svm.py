# -*- coding: utf-8 -*-
"""ML_project_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WZaLl8w64zrSWRNJemtqnPmkJNjO6Gr_
"""

###########
#answers to questions are given in same text cell as the questions
###########

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
# PyTorch
import torch
import torch.nn as nn
import torch.nn.functional as F
# To Read Data
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
# To Interpret results & obtain plots
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score
import matplotlib.pyplot as plt
from zipfile import ZipFile

with ZipFile('/content/drive/MyDrive/cs464_hw3.zip', 'r') as zipObj:
  zipObj.extractall()
# You could add your own libraries form Python Standard Library in this cell. Any other external libraries are not allowed.

class CatsDataset(Dataset):
    # TODO:
    # Define constructor for SVHNDataset class
    # HINT: You can pass processed data samples and their ground truth values as parameters 
    def __init__(self, data, labels): # you are free to change parameters
        self.data = torch.FloatTensor(data.astype('float')).cuda()
        self.labels = torch.FloatTensor(labels.astype('int')).cuda()
        #self.data = data
        #self.labels = labels
    '''This function should return sample count in the dataset'''
    def __len__(self):
        return self.data.shape[0]

    '''This function should return a single sample and its ground truth value from the dataset corresponding to index parameter '''
    def __getitem__(self, index):
        return self.data[index,:], self.labels[index]

import os
from  sklearn.utils import shuffle
import cv2

#mode 0 for CNN mode 1 for MLP
def get_dataset(root, mode):
#root = "data"
#mode = 1
  lab = 0
  processed_data = []
  processed_labels = []
  for filename in os.listdir(root):
    for newname in os.listdir(os.path.join(root,filename)):
      img = np.array(Image.open(os.path.join(root, os.path.join(filename, newname))))
      img = cv2.resize(img, (64,64)) 
      if mode == 1:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = img.flatten().reshape(4096,)
      img = img/255
      
      processed_data.append(img)
      processed_labels.append(lab)
    lab+=1


  processed_data, processed_labels = shuffle(processed_data, processed_labels)
  train_dataset = processed_data[:int((np.array(processed_data).shape[0]*7)/10)]
  val_dataset = processed_data[int((np.array(processed_data).shape[0]*7)/10):int((np.array(processed_data).shape[0]*8)/10)] 
  test_dataset = processed_data[int((np.array(processed_data).shape[0]*8)/10):]
  train_labels = processed_labels[:int((np.array(processed_labels).shape[0]*7)/10)]
  val_labels = processed_labels[int((np.array(processed_labels).shape[0]*7)/10):int((np.array(processed_labels).shape[0]*8)/10)]
  test_labels = processed_labels[int((np.array(processed_labels).shape[0]*8)/10):]

  return train_dataset, val_dataset, test_dataset, train_labels, val_labels, test_labels
    



      




    # TODO: 
    # Read dataset files
    # Resize images as 64x64 so that you have consistent data. You can use img.resize((64,64)) from PIL.Image.
    # Construct training, validation and test sets
    # Normalize datasets
    
    #return train_dataset, val_dataset, test_dataset#



#train_dataset, val_dataset, test_dataset = get_dataset("data",)

class ConvNet(nn.Module):
    '''Define your neural network'''
    def __init__(self): # you can add any additional parameters you want 
      super(ConvNet, self).__init__()
      self.conv1 = nn.Conv2d(in_channels=3,out_channels=8,stride=1,padding=0,kernel_size=3)
      self.conv2 = nn.Conv2d(in_channels=8,out_channels=16,stride=1,padding=0,kernel_size=3)
      self.conv3 = nn.Conv2d(in_channels=16,out_channels=32,stride=1,padding=0,kernel_size=5)
      self.conv4 = nn.Conv2d(in_channels=32,out_channels=32,stride=1,padding=0,kernel_size=5)
      self.mp = nn.MaxPool2d(2)
      self.fc = nn.Linear(512, 13)
    # TODO:
    # You should create your neural network here

     
    def forward(self, X): # you can add any additional parameters you want
      in_size = X.size(0)
      X = F.relu(self.conv1(X))
      X = F.relu(self.mp(self.conv2(X)))
      X = F.relu(self.mp(self.conv3(X)))
      X = F.relu(self.mp(self.conv4(X)))
      #(print(X.size()))
      X = torch.flatten(X, start_dim=1)  # flatten the tensor
      #(print(X.size()))
      X = self.fc(X)
      #(print(X.size()))
      return torch.softmax(X,dim=1)
      #return F.softmax(X)
    # TODO:
    # Forward propagation implementation should be here

# HINT: note that your training time should not take many days.

# TODO:
# Pick your hyper parameters
max_epoch = 400
train_batch = 32
test_batch = 32
learning_rate = 0.0001

use_gpu = torch.cuda.is_available()

train_dataset, val_dataset, test_dataset, train_labels, val_labels, test_labels = get_dataset("data", 2)
# Create train dataset loader
train_dataset = np.array(train_dataset)
train_labels = np.array(train_labels)
test_dataset = np.array(test_dataset)
test_labels = np.array(test_labels)
val_dataset = np.array(val_dataset)
val_labels = np.array(val_labels)
train_data = CatsDataset(train_dataset, train_labels)
trainloader = DataLoader(train_data, batch_size=train_batch, shuffle=True)
# Create validation dataset loader
valid_data = CatsDataset(val_dataset, val_labels)
val_loader = DataLoader(valid_data, batch_size=test_batch)
# Create test dataset loader
test_data = CatsDataset(test_dataset, test_labels)
testloader = DataLoader(test_data, batch_size=test_batch)

# initialize your network
model = ConvNet()
if use_gpu:
  model.cuda()

# define your loss function
criterion = nn.MSELoss()

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-03) # you can play with  weight_decay as well
    
# start training
# for each epoch calculate validation performance
# save best model according to validation performance

lLoss = 0
FinalLossTrain=[]
FinalLossValid=[]
FinalAccTrain=[]
FinalAccValid=[]
check_for = 0
for epoch in range(max_epoch):
  #print("iteration start: ", check_for)
  ###############################################333
  model=model.train()
  j=0
  
  lossez = np.zeros(train_data.__len__())
  predicteds=[]
  actuals = []
  for batch_idx, (data, labels) in enumerate(trainloader):
    
    if use_gpu:
      data = data.cuda()
    optimizer.zero_grad()
    y_pred = model(data.permute(0, 3, 1, 2))
    #y_pred = torch.max(y_pred)

    one_hot = torch.nn.functional.one_hot(labels.to(torch.int64),13)
    loss  = criterion(y_pred, one_hot.to(torch.float))
    #loss  = criterion(y_pred, labels.unsqueeze(1))

    loss.backward()
    optimizer.step()
    predicted = np.array((y_pred).cpu().detach())
    actual = np.array(labels[:].unsqueeze(1).cpu().detach())
    
    kia = 0
    nahi = 0
    
    for kuch in actual:     
      if actual[kia] == np.argmax(predicted[kia]):
        nahi += 1
      kia += 1
    accuracies = nahi/kia
    losses = np.mean(np.array(loss.cpu().detach()))
    
    for x in predicted:
      predicteds.append(np.argmax(x))
    for x in actual:
      actuals.append(x)
    
    #lossez[j*train_batch:(j+1)*train_batch] = (np.array(criterion(y_pred, labels.unsqueeze(1)).cpu().detach()))
    lossez[j*train_batch:(j+1)*train_batch] = losses
    j=j+1 

  kia = 0
  nahi = 0
  #print(lossez)
  for kuch in actuals:
    if actuals[kia][0] == predicteds[kia]:
      nahi += 1
    kia += 1
  acc = nahi/kia

  #print("iteration end: ", check_for," acc: ", acc*100)
  ########################################################
#    iterate over training batches
#    ...

#    Validation
  model = model.eval()
  criterions = nn.MSELoss()
  lossezV = np.zeros(valid_data.__len__())
  predictedsV=[]
  actualsV = []
  j=0
  with torch.no_grad():
    for batch_idx, (data, labels) in enumerate(val_loader):
      #print(labels)
      y_pred = model(data.permute(0, 3, 1, 2))
      predicted =np.array((y_pred).cpu().detach())
      actual = np.array(labels[:].unsqueeze(1).cpu().detach())
      for x in predicted:
        predictedsV.append(np.argmax(x))
      for x in actual:
        actualsV.append(x)
      

      one_hotV = torch.nn.functional.one_hot(labels.to(torch.int64),13)
      lossV  = criterion(y_pred, one_hotV.to(torch.float))
      lossesV = np.mean(np.array(lossV.cpu().detach()))
      lossezV[j*train_batch:(j+1)*train_batch] = lossesV

      #lossezV[j*train_batch:(j+1)*train_batch] = (np.array(criterion(y_pred, labels.unsqueeze(1)).cpu().detach()))
      j=j+1   
      kia = 0
      nahi = 0
      
      for kuch in actual:
        if actual[kia] == np.argmax(predicted[kia]):
          nahi += 1
        kia += 1
      accuracies = nahi/kia
    
    kia = 0
    nahi = 0
    
    for kuch in actualsV:
      if actualsV[kia][0] == predictedsV[kia]:
        nahi += 1
      kia += 1
    accV = nahi/kia

  FinalLossTrain.append(np.mean(lossez)*100)
  FinalLossValid.append(np.mean(lossezV)*100)
  FinalAccTrain.append(acc)
  FinalAccValid.append(accV)
#     iterate over validation batches
  if accV > lLoss:
    torch.save(model, 'best_path_cnn.pth')
    lLoss = accV

  
  #print("iteration end: ", check_for," accV: ", accV*100)
  check_for += 1
# plot losses vs epoch 
# ...
# plt.show()
plt.figure()
plt.plot(FinalLossTrain, label='train loss')
plt.plot(FinalLossValid, label='validation loss')
plt.legend(['Training loss', 'Validation Loss'],fontsize=10)
#print(FinalLossTrain)
#print(FinalLossValid)
# plot accuracies vs epoch
# ...
# plt.show()
plt.figure()
plt.plot(FinalAccTrain, label='train acc')
plt.plot(FinalAccValid, label='validation acc')
plt.legend(['Training accuracy', 'Validation accuracy'],fontsize=10)

# for epoch in range(max_epoch):
#    model=model.train()
#    iterate over training batches
#    ...

#    Validation
#    model = model.eval()
#    with torch.no_grad():
#     iterate over validation batches
#    if ???????:
#       torch.save(model, best_path)

# plot losses vs epoch 
# ...
# plt.show()

# plot accuracies vs epoch
# ...
# plt.show()

